{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(SimpleNet, self).__init__()\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(12288, 84)  \n",
    "        self.fc2 = nn.Linear(84, 50)     \n",
    "        self.fc3 = nn.Linear(50, 2)      \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        x = x.view(-1, 12288)             \n",
    "        x = F.relu(self.fc1(x))           \n",
    "        x = F.relu(self.fc2(x))           \n",
    "\n",
    "        # x = F.softmax(self.fc3(x), dim=1) \n",
    "\n",
    "        x = self.fc3(x)  \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "\n",
    "optimizer = optim.Adam(simplenet.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial learning rate:\", optimizer.param_groups[0][\"lr\"])  # Output: 0.001\n",
    "\n",
    "optimizer.param_groups[0][\"lr\"] = 0.01\n",
    "\n",
    "print(\"Updated learning rate:\", optimizer.param_groups[0][\"lr\"])  # Output: 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create different learning rates for different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([\n",
    "    {'params': model[0].parameters(), 'lr': 0.001},  \n",
    "    {'params': model[1].parameters(), 'lr': 0.01}    \n",
    "])\n",
    "\n",
    "print(\"Learning rate for layer 1:\", optimizer.param_groups[0]['lr'])  \n",
    "print(\"Learning rate for layer 2:\", optimizer.param_groups[1]['lr']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
